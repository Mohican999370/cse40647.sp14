{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function Definitions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Everaldo Aguiar & Reid Johnson\n",
      "#\n",
      "# Modified from:\n",
      "# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n",
      "#\n",
      "# SciPy function to compute and extract association rules from a given frequent itemset \n",
      "# generated by the Apriori algorithm.\n",
      "#\n",
      "# The Apriori algorithm is defined by Agrawal and Srikant in:\n",
      "# Fast algorithms for mining association rules\n",
      "# Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994\n",
      "\n",
      "def create_candidates(dataset):\n",
      "    '''Creates a list of candidate 1-itemsets from a list of transactions.\n",
      "\n",
      "    Args:\n",
      "      dataset (array): The dataset (an array of transactions) from which to generate \n",
      "        candidate itemsets.\n",
      "\n",
      "    Returns:\n",
      "      frozenset mapping of c1: The list of candidate itemsets (c1) passed as a \n",
      "        frozenset (a set that is immutable and hashable).\n",
      "\n",
      "    '''\n",
      "    c1 = [] # list of all items in the database of transactions\n",
      "    for transaction in dataset:\n",
      "        for item in transaction:\n",
      "            if not [item] in c1:\n",
      "                c1.append([item])\n",
      "    c1.sort()\n",
      "\n",
      "    # map c1 to a frozenset because it will be the key of a dictionary\n",
      "    return map(frozenset, c1)\n",
      "\n",
      "def support_prune(dataset, candidates, min_support):\n",
      "    '''Returns all candidate itemsets that meet a minimum support threshold.\n",
      "\n",
      "    By the apriori principle, if an itemset is frequent, then all of its subsets must\n",
      "    also be frequent. As a result, we can perform support-based pruning to systemically\n",
      "    control the exponential growth of candidate itemsets. Thus, itemsets that do not \n",
      "    meet the minimum support level are pruned from the input list of itemsets (dataset).\n",
      "\n",
      "    Args:\n",
      "      dataset (array): The dataset (an array of transactions) from which to generate \n",
      "        candidate itemsets.\n",
      "      candidates (frozenset): The list of candidate itemsets.\n",
      "      min_support (float): The minimum support threshold.\n",
      "\n",
      "    Returns:\n",
      "      retlist: The list of frequent itemsets.\n",
      "      support_data: The support data for all candidate itemsets.\n",
      "\n",
      "    '''\n",
      "    sscnt = {} # set for support counts\n",
      "    for tid in dataset:\n",
      "        for can in candidates:\n",
      "            if can.issubset(tid):\n",
      "                sscnt.setdefault(can, 0)\n",
      "                sscnt[can] += 1\n",
      " \n",
      "    num_items = float(len(dataset)) # total number of transactions in the dataset\n",
      "    retlist = [] # array for unpruned itemsets\n",
      "    support_data = {} # set for support data for corresponding itemsets\n",
      "    for key in sscnt:\n",
      "        # calculate the support of itemset key\n",
      "        support = sscnt[key] / num_items\n",
      "        if support >= min_support:\n",
      "            retlist.insert(0, key)\n",
      "        support_data[key] = support\n",
      "\n",
      "    return retlist, support_data\n",
      "\n",
      "def apriori_gen(freq_sets, k):\n",
      "    '''Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
      "\n",
      "    This operation generates new candidate k-itemsets based on the frequent (k-1)-itemsets\n",
      "    found in the previous iteration. The candidate generation procedure merges a pair of\n",
      "    frequent (k-1)-itemsets only if their first k-2 items are identical.\n",
      "\n",
      "    Args:\n",
      "      freq_sets (array): The list of frequent (k-1)-itemsets.\n",
      "      k (int): The list of candidate itemsets.\n",
      "\n",
      "    Returns:\n",
      "      retlist: The list of joint candidate itemsets.\n",
      "\n",
      "    '''\n",
      "    retList = []\n",
      "    lenLk = len(freq_sets)\n",
      "    for i in range(lenLk):\n",
      "        for j in range(i+1, lenLk):\n",
      "            L1 = list(freq_sets[i])[:k-2]\n",
      "            L2 = list(freq_sets[j])[:k-2]\n",
      "            L1.sort()\n",
      "            L2.sort()\n",
      "            if L1 == L2:\n",
      "                retList.append(freq_sets[i] | freq_sets[j])\n",
      "\n",
      "    return retList\n",
      "\n",
      "def apriori(dataset, min_support=0.5):\n",
      "    '''Generates a list of candidate itemsets.\n",
      "    \n",
      "    The Apriori algorihtm will iteratively generate new candidate k-itemsets using the \n",
      "    frequent (k-1)-itemsets found in the previous iteration.\n",
      "\n",
      "    Args:\n",
      "      dataset (array): The dataset (an array of transactions) from which to generate \n",
      "        candidate itemsets.\n",
      "      min_support (float): The minimum support threshold. Defaults to 0.5.\n",
      "\n",
      "    Returns:\n",
      "      L: The list of frequent itemsets.\n",
      "      support_data: The support data for all candidate itemsets.\n",
      "\n",
      "    '''\n",
      "    C1 = create_candidates(dataset)\n",
      "    D = map(set, dataset)\n",
      "    L1, support_data = support_prune(D, C1, min_support)\n",
      "    L = [L1]\n",
      "    k = 2\n",
      "    while (len(L[k - 2]) > 0):\n",
      "        Ck = apriori_gen(L[k-2], k)\n",
      "        Lk, supK = support_prune(D, Ck, min_support)\n",
      "        support_data.update(supK)\n",
      "        L.append(Lk)\n",
      "        k += 1\n",
      " \n",
      "    return L, support_data\n",
      "\n",
      "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.7):\n",
      "    '''Generates a set of candidate rules.\n",
      "\n",
      "    Args:\n",
      "      freq_set (frozenset): The complete list of frequent itemsets.\n",
      "      H (set):  A list of frequent itemsets (of a particular length).\n",
      "      support_data (array): The support data for all candidate itemsets.\n",
      "      rules (array): A potentially incomplete set of candidate rules above the minimum \n",
      "        confidence threshold.\n",
      "      min_confidence (float): The minimum confidence threshold. Defaults to 0.7.\n",
      "\n",
      "    '''\n",
      "    m = len(H[0])\n",
      "    if (len(freq_set) > (m+1)):\n",
      "        Hmp1 = apriori_gen(H, m+1)\n",
      "        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence)\n",
      "        if len(Hmp1) > 1:\n",
      "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence)\n",
      "\n",
      "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.7):\n",
      "    '''Evaluates the generated rules.\n",
      "\n",
      "    A measurement for quantifying the goodness of association rules is the confidence.\n",
      "    The confidence for a rule P implies H (P -> H) is defined as the support for P and H\n",
      "    divided by the support for P (support (P|H) / support(P)), where the | symbol denotes\n",
      "    the set union (thus P|H means all the items in set P or in set H).\n",
      "\n",
      "    To calculate the confidence, we iterate through the frequent itemsets and associated\n",
      "    support data. For each frequent itemset, we divide the support of the itemset by the\n",
      "    support of the antecedent (left-hand-side of the rule).\n",
      "\n",
      "    Args:\n",
      "      freq_set (frozenset): The complete list of frequent itemsets.\n",
      "      H (set): A list of frequent itemsets (of a particular length).\n",
      "      min_support (float): The minimum support threshold.\n",
      "      rules (array): A potentially incomplete set of candidate rules above the minimum \n",
      "        confidence threshold.\n",
      "      min_confidence (float): The minimum confidence threshold. Defaults to 0.7.\n",
      "\n",
      "    Returns:\n",
      "      pruned_H: The set of candidate rules above the minimum confidence threshold.\n",
      "\n",
      "    '''\n",
      "    pruned_H = []\n",
      "    for conseq in H:\n",
      "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
      "        if conf >= min_confidence:\n",
      "            print freq_set - conseq, '--->', conseq, 'conf:', conf\n",
      "            rules.append((freq_set - conseq, conseq, conf))\n",
      "            pruned_H.append(conseq)\n",
      "\n",
      "    return pruned_H\n",
      "\n",
      "def generate_rules(L, support_data, min_confidence=0.7):\n",
      "    '''Generates a set of candidate rules from a list of frequent itemsets.\n",
      "\n",
      "    For each frequent itemset, we calculate the confidence of using a particular item\n",
      "    as the rule consequent (right-hand-side of the rule). By testing and merging the \n",
      "    remaining rules, we recursively create a list of pruned rules.\n",
      "\n",
      "    Args:\n",
      "      L (array): A list of frequent itemsets.\n",
      "      support_data (array): The corresponding support data for the frequent itemsets (L).\n",
      "      min_confidence (float): The minimum confidence threshold. Defaults to 0.7.\n",
      "\n",
      "    Returns:\n",
      "      rules: The set of candidate rules above the minimum confidence threshold.\n",
      "\n",
      "    '''\n",
      "    rules = []\n",
      "    for i in range(1, len(L)):\n",
      "        for freq_set in L[i]:\n",
      "            H1 = [frozenset([item]) for item in freq_set]\n",
      "            #print \"freq_set\", freq_set, 'H1', H1\n",
      "            if (i > 1):\n",
      "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence)\n",
      "            else:\n",
      "                calc_confidence(freq_set, H1, support_data, rules, min_confidence)\n",
      "\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generating a Test Dataset and Testing the Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we define a function that loads an example of market basket transactions for testing purposes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_dataset():\n",
      "    return [['Bread', 'Milk'], \n",
      "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
      "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
      "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
      "            ['Bread', 'Milk', 'Diapers', 'Coke']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we load the market basket transactions dataset (a list of lists) and print the transactions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = load_dataset() # list of transactions; each transaction is a list of items\n",
      "print dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['Bread', 'Milk'], ['Bread', 'Diapers', 'Beer', 'Eggs'], ['Milk', 'Diapers', 'Beer', 'Coke'], ['Bread', 'Milk', 'Diapers', 'Beer'], ['Bread', 'Milk', 'Diapers', 'Coke']]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several aspects to the Apriori algorithm. First, the algorithm initially makes a single pass over the dataset to determine the support of each item (here, performed during the execution of the support_prune function on the candidate 1-itemsets returned by the create_candidates function). Upon completion of this step, the set of all frequent 1-itemsets will be known. Next, the algorithm will iteratively generate new candidate k-itemsets using the frequent (k-1)-itemsets found in the previous iteration (here, via the apriori_gen function).\n",
      "\n",
      "Using our code, we could begin this process by creating the initial candidates (and mapping our original sets to a Python set). Note that a frozenset is simply a type of set object in Python. Also, we import and use pprint to format the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint\n",
      "\n",
      "# Generate candidate itemsets\n",
      "C1 = create_candidates(dataset) # candidate 1-itemsets\n",
      "D = map(set, dataset) # set of transactions; each transaction is a list of items\n",
      "\n",
      "pprint.pprint(C1)\n",
      "print\n",
      "pprint.pprint(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[frozenset(['Beer']),\n",
        " frozenset(['Bread']),\n",
        " frozenset(['Coke']),\n",
        " frozenset(['Diapers']),\n",
        " frozenset(['Eggs']),\n",
        " frozenset(['Milk'])]\n",
        "\n",
        "[set(['Bread', 'Milk']),\n",
        " set(['Beer', 'Bread', 'Diapers', 'Eggs']),\n",
        " set(['Beer', 'Coke', 'Diapers', 'Milk']),\n",
        " set(['Beer', 'Bread', 'Diapers', 'Milk']),\n",
        " set(['Bread', 'Coke', 'Diapers', 'Milk'])]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we could generate the frequent 1-itemsets by pruning candidate 1-itemsets that do not meet the minimum support:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets\n",
      "F1, support_data = support_prune(D, C1, 0.6)\n",
      "\n",
      "pprint.pprint(F1)\n",
      "print\n",
      "pprint.pprint(support_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[frozenset(['Milk']),\n",
        " frozenset(['Bread']),\n",
        " frozenset(['Beer']),\n",
        " frozenset(['Diapers'])]\n",
        "\n",
        "{frozenset(['Eggs']): 0.2,\n",
        " frozenset(['Diapers']): 0.8,\n",
        " frozenset(['Beer']): 0.6,\n",
        " frozenset(['Bread']): 0.8,\n",
        " frozenset(['Coke']): 0.4,\n",
        " frozenset(['Milk']): 0.8}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we could iteratively generate the remaining frequent itemsets via the apriori_gen function. However, our code wraps the entire process into one function (apriori), which internally executes create_candidates, support_prune, and apriori_gen. We can simply input the initial dataset into this function (along with a minimum support threshold) and it will return a list of all the frequent itemsets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate all the frequent itemsets\n",
      "F, support_data = apriori(dataset, min_support=0.6)\n",
      "pprint.pprint(F)\n",
      "#print support_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[frozenset(['Milk']),\n",
        "  frozenset(['Bread']),\n",
        "  frozenset(['Beer']),\n",
        "  frozenset(['Diapers'])],\n",
        " [frozenset(['Beer', 'Diapers']),\n",
        "  frozenset(['Bread', 'Diapers']),\n",
        "  frozenset(['Diapers', 'Milk']),\n",
        "  frozenset(['Bread', 'Milk'])],\n",
        " []]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a frequent itemset (here, extracted by the Apriori algorithm), we can generate the association rules with high support and confidence (via the generate_rules function):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate the association rules from a given frequent itemset\n",
      "H = generate_rules(F, support_data)\n",
      "#pprint.pprint(H)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "frozenset(['Diapers']) ---> frozenset(['Beer']) conf: 0.75\n",
        "frozenset(['Beer']) ---> frozenset(['Diapers']) conf: 1.0\n",
        "frozenset(['Bread']) ---> frozenset(['Diapers']) conf: 0.75\n",
        "frozenset(['Diapers']) ---> frozenset(['Bread']) conf: 0.75\n",
        "frozenset(['Diapers']) ---> frozenset(['Milk']) conf: 0.75\n",
        "frozenset(['Milk']) ---> frozenset(['Diapers']) conf: 0.75\n",
        "frozenset(['Bread']) ---> frozenset(['Milk']) conf: 0.75\n",
        "frozenset(['Milk']) ---> frozenset(['Bread']) conf: 0.75\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}
