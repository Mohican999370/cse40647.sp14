{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      " \n",
      "iris = load_iris()\n",
      "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
      "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
      "df['species'] = pd.Categorical(iris.target, iris.target_names)\n",
      "df.head()\n",
      " \n",
      "train, test = df[df['is_train']==True], df[df['is_train']==False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "features = df.columns[:4]\n",
      "clf = DecisionTreeClassifier()\n",
      "y, _ = pd.factorize(train['species'])\n",
      "clf.fit(train[features], y)\n",
      " \n",
      "preds = iris.target_names[clf.predict(test[features])]\n",
      "pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th>preds</th>\n",
        "      <th>setosa</th>\n",
        "      <th>versicolor</th>\n",
        "      <th>virginica</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>actual</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>setosa</th>\n",
        "      <td> 13</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>versicolor</th>\n",
        "      <td>  0</td>\n",
        "      <td> 10</td>\n",
        "      <td>  0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>virginica</th>\n",
        "      <td>  0</td>\n",
        "      <td>  2</td>\n",
        "      <td> 11</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>3 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "preds       setosa  versicolor  virginica\n",
        "actual                                   \n",
        "setosa          13           0          0\n",
        "versicolor       0          10          0\n",
        "virginica        0           2         11\n",
        "\n",
        "[3 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "features = df.columns[:4]\n",
      "clf = RandomForestClassifier(n_jobs=2)\n",
      "y, _ = pd.factorize(train['species'])\n",
      "clf.fit(train[features], y)\n",
      " \n",
      "preds = iris.target_names[clf.predict(test[features])]\n",
      "pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th>preds</th>\n",
        "      <th>setosa</th>\n",
        "      <th>versicolor</th>\n",
        "      <th>virginica</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>actual</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>setosa</th>\n",
        "      <td> 13</td>\n",
        "      <td>  0</td>\n",
        "      <td>  0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>versicolor</th>\n",
        "      <td>  0</td>\n",
        "      <td> 10</td>\n",
        "      <td>  0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>virginica</th>\n",
        "      <td>  0</td>\n",
        "      <td>  2</td>\n",
        "      <td> 11</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>3 rows \u00d7 3 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "preds       setosa  versicolor  virginica\n",
        "actual                                   \n",
        "setosa          13           0          0\n",
        "versicolor       0          10          0\n",
        "virginica        0           2         11\n",
        "\n",
        "[3 rows x 3 columns]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)\n",
      "\n",
      "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()                             \n",
      "\n",
      "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()                             \n",
      "\n",
      "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
      "scores = cross_val_score(clf, X, y)\n",
      "print scores.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.980400139594\n",
        "0.999600049987"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pylab as pl\n",
      "\n",
      "from sklearn import clone\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier)\n",
      "from sklearn.externals.six.moves import xrange\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Parameters\n",
      "n_classes = 3\n",
      "n_estimators = 30\n",
      "plot_colors = \"ryb\"\n",
      "cmap = pl.cm.RdYlBu\n",
      "plot_step = 0.02  # fine step width for decision surface contours\n",
      "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
      "RANDOM_SEED = 13  # fix the seed on each iteration\n",
      "\n",
      "# Load data\n",
      "iris = load_iris()\n",
      "\n",
      "plot_idx = 1\n",
      "\n",
      "models = [DecisionTreeClassifier(max_depth=None),\n",
      "          RandomForestClassifier(n_estimators=n_estimators),\n",
      "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
      "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
      "                             n_estimators=n_estimators)]\n",
      "\n",
      "for pair in ([0, 1], [0, 2], [2, 3]):\n",
      "    for model in models:\n",
      "        # We only take the two corresponding features\n",
      "        X = iris.data[:, pair]\n",
      "        y = iris.target\n",
      "\n",
      "        # Shuffle\n",
      "        idx = np.arange(X.shape[0])\n",
      "        np.random.seed(RANDOM_SEED)\n",
      "        np.random.shuffle(idx)\n",
      "        X = X[idx]\n",
      "        y = y[idx]\n",
      "\n",
      "        # Standardize\n",
      "        mean = X.mean(axis=0)\n",
      "        std = X.std(axis=0)\n",
      "        X = (X - mean) / std\n",
      "\n",
      "        # Train\n",
      "        clf = clone(model)\n",
      "        clf = model.fit(X, y)\n",
      "\n",
      "        scores = clf.score(X, y)\n",
      "        # Create a title for each column and the console by using str() and\n",
      "        # slicing away useless parts of the string\n",
      "        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
      "        model_details = model_title\n",
      "        if hasattr(model, \"estimators_\"):\n",
      "            model_details += \" with {} estimators\".format(len(model.estimators_))\n",
      "        print model_details + \" with features\", pair, \"has a score of\", scores\n",
      "\n",
      "        pl.subplot(3, 4, plot_idx)\n",
      "        if plot_idx <= len(models):\n",
      "            # Add a title at the top of each column\n",
      "            pl.title(model_title)\n",
      "\n",
      "        # Now plot the decision boundary using a fine mesh as input to a\n",
      "        # filled contour plot\n",
      "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
      "                             np.arange(y_min, y_max, plot_step))\n",
      "\n",
      "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
      "        # decision surfaces of the ensemble of classifiers\n",
      "        if isinstance(model, DecisionTreeClassifier):\n",
      "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "            Z = Z.reshape(xx.shape)\n",
      "            cs = pl.contourf(xx, yy, Z, cmap=cmap)\n",
      "        else:\n",
      "            # Choose alpha blend level with respect to the number of estimators\n",
      "            # that are in use (noting that AdaBoost can use fewer estimators\n",
      "            # than its maximum if it achieves a good enough fit early on)\n",
      "            estimator_alpha = 1.0 / len(model.estimators_)\n",
      "            for tree in model.estimators_:\n",
      "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "                Z = Z.reshape(xx.shape)\n",
      "                cs = pl.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
      "\n",
      "        # Build a coarser grid to plot a set of ensemble classifications\n",
      "        # to show how these are different to what we see in the decision\n",
      "        # surfaces. These points are regularly space and do not have a black outline\n",
      "        xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),\n",
      "                                             np.arange(y_min, y_max, plot_step_coarser))\n",
      "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)\n",
      "        cs_points = pl.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors=\"none\")\n",
      "\n",
      "        # Plot the training points, these are clustered together and have a\n",
      "        # black outline\n",
      "        for i, c in zip(xrange(n_classes), plot_colors):\n",
      "            idx = np.where(y == i)\n",
      "            pl.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n",
      "                       cmap=cmap)\n",
      "\n",
      "        plot_idx += 1  # move on to the next plot in sequence\n",
      "\n",
      "pl.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
      "pl.axis(\"tight\")\n",
      "\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}
