{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Artificial Neural Networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Artificial neural networks are computational models inspired by animals' central nervous systems that are capable of machine learning and pattern recognition. They are usually presented as systems of interconnected \"neurons\" that can compute values from inputs by feeding information through the network."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Reid Johnson\n",
      "#\n",
      "# Modified from:\n",
      "# Daniel Rodriguez (http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/)\n",
      "#\n",
      "# Functions to generate an artifical neural network with one hidden layer.\n",
      "#\n",
      "# The perceptron algorithm is defined by Rosenblatt in:\n",
      "# The Perceptron: A Probalistic Model for Information Storage and Organization in the Brain.\n",
      "# Psychological Review 65 (6): 386\u2013408. 1958.\n",
      "#\n",
      "# The backpropogation algorithm is defined by Werbos in:\n",
      "# Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. 1975.\n",
      "\n",
      "import numpy as np\n",
      "from scipy import optimize\n",
      "from __future__ import division\n",
      "\n",
      "class NN_1HL(object):\n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):\n",
      "        self.reg_lambda = reg_lambda # weight for the logistic regression cost\n",
      "        self.epsilon_init = epsilon_init # step size for backpropagation gradient checking\n",
      "        self.hidden_layer_size = hidden_layer_size # size of the hidden layer\n",
      "        self.activation_func = self.sigmoid # activation function\n",
      "        self.activation_func_prime = self.sigmoid_prime # derivative of the activation function\n",
      "        self.method = opti_method # optimization method\n",
      "        self.maxiter = maxiter # maximum number of iterations\n",
      "\n",
      "    # Logistic function.\n",
      "    def sigmoid(self, z):\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "\n",
      "    # Derivative of the logistic function.\n",
      "    def sigmoid_prime(self, z):\n",
      "        sig = self.sigmoid(z)\n",
      "\n",
      "        return sig * (1-sig)\n",
      "\n",
      "    # Sum of squared values.\n",
      "    def sumsqr(self, a):\n",
      "        return np.sum(a**2)\n",
      "\n",
      "    def rand_init(self, l_in, l_out):\n",
      "        return np.random.rand(l_out, l_in+1) * 2 * self.epsilon_init - self.epsilon_init\n",
      "\n",
      "    # Pack thetas into a one-dimensional array.\n",
      "    def pack_thetas(self, t1, t2):\n",
      "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
      "\n",
      "    # Unpack thetas into a multi-dimensional array.\n",
      "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "        t1_start = 0\n",
      "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "\n",
      "        return t1, t2\n",
      "\n",
      "    # Forward propogation.\n",
      "    def _forward(self, X, t1, t2):\n",
      "        m = X.shape[0]\n",
      "        ones = None\n",
      "        if len(X.shape) == 1:\n",
      "            ones = np.array(1).reshape(1,)\n",
      "        else:\n",
      "            ones = np.ones(m).reshape(m,1)\n",
      "\n",
      "        # Input layer\n",
      "        a1 = np.hstack((ones, X))\n",
      "\n",
      "        # Hidden Layer\n",
      "        z2 = np.dot(t1, a1.T)\n",
      "        a2 = self.activation_func(z2)\n",
      "        a2 = np.hstack((ones, a2.T))\n",
      "\n",
      "        # Output layer\n",
      "        z3 = np.dot(t2, a2.T)\n",
      "        a3 = self.activation_func(z3)\n",
      "\n",
      "        return a1, z2, a2, z3, a3\n",
      "\n",
      "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        _, _, _, _, h = self._forward(X, t1, t2)\n",
      "        costPositive = -Y * np.log(h).T\n",
      "        costNegative = (1 - Y) * np.log(1 - h).T\n",
      "        cost = costPositive - costNegative\n",
      "        J = np.sum(cost) / m\n",
      "\n",
      "        if reg_lambda != 0:\n",
      "            t1f = t1[:, 1:]\n",
      "            t2f = t2[:, 1:]\n",
      "            reg = (self.reg_lambda / (2*m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))\n",
      "            J = J + reg\n",
      "\n",
      "        return J\n",
      "\n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        Delta1, Delta2 = 0, 0\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "\n",
      "            # Backprop\n",
      "            d3 = a3 - Y[i, :].T\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
      "\n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "\n",
      "        Theta1_grad = (1 / m) * Delta1\n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "\n",
      "        if reg_lambda != 0:\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "\n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "\n",
      "    # Fit the artificial neural network given predictor(s) X and target y.\n",
      "    def fit(self, X, y):\n",
      "        num_features = X.shape[0]\n",
      "        input_layer_size = X.shape[1]\n",
      "        num_labels = len(set(y))\n",
      "\n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "\n",
      "        options = {'maxiter': self.maxiter}\n",
      "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
      "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
      "\n",
      "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "\n",
      "    # Predict labels with the fitted model on predictor(s) X.\n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "\n",
      "    # Predict probabilities with the fitted model on predictor(s) X.\n",
      "    def predict_proba(self, X):\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we will demonstrate artificial neural networks (ANNs) by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding method available via scikit-learn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "\n",
      "label_encode = True\n",
      "n_folds = 5\n",
      "\n",
      "# Load the Iris flower dataset\n",
      "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
      "iris = pd.read_csv(fileURL, \n",
      "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
      "                   header=None)\n",
      "iris = iris.dropna()\n",
      "\n",
      "X = np.array(iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]) # features\n",
      "y = iris['Species'] # class\n",
      "\n",
      "if label_encode:\n",
      "    # Transform string (nominal) output to numeric\n",
      "    labels = preprocessing.LabelEncoder().fit_transform(y)\n",
      "else:\n",
      "    labels = y\n",
      "\n",
      "# Generate k stratified folds of the data.\n",
      "skf = list(cross_validation.StratifiedKFold(labels, n_folds))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import timeit\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Generate classification objects.\n",
      "nn = NN_1HL()\n",
      "rfc = RandomForestClassifier(n_estimators=50)\n",
      "\n",
      "# Generate arrays for meta-level training and testing sets, which are n x len(clfs).\n",
      "scores_nn = np.zeros(n_folds) # scores for nn\n",
      "scores_rfc = np.zeros(n_folds) # scores for rfc\n",
      "\n",
      "print 'Training classifiers...'\n",
      "# Iterate over the folds, each with training set and validation set indicies.\n",
      "for i, (train_index, test_index) in enumerate(skf):\n",
      "    print '  Fold [%s]' % (i)\n",
      "\n",
      "    # Generate the training set for the fold.\n",
      "    X_train = X[train_index]\n",
      "    y_train = labels[train_index]\n",
      "\n",
      "    # Generate the testing set for the fold.\n",
      "    X_test = X[test_index]\n",
      "    y_test = labels[test_index]\n",
      "\n",
      "    # Train the models on the training set.\n",
      "    # We time the training using the built-in timeit magic function.\n",
      "    print '    Neural Network: ',\n",
      "    %timeit nn.fit(X_train, y_train)\n",
      "    print '    Random Forest: ',\n",
      "    %timeit rfc.fit(X_train, y_train)\n",
      "\n",
      "    # Evaluate the models on the testing set.\n",
      "    scores_nn[i] = metrics.accuracy_score(y_test, nn.predict(X_test))\n",
      "    scores_rfc[i] = metrics.accuracy_score(y_test, rfc.predict(X_test))\n",
      "print 'Done training classifiers.'\n",
      "print\n",
      "\n",
      "# The mean of the scores on the testing set.\n",
      "print 'Artificial Neural Network Accuracy = %s' % (scores_nn.mean(axis=0))\n",
      "print 'Random Forest Accuracy = %s' % (scores_rfc.mean(axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training classifiers...\n",
        "  Fold [0]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.53 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 48 ms per loop\n",
        "  Fold [1]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 1.82 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 57 ms per loop\n",
        "  Fold [2]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.6 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 40.5 ms per loop\n",
        "  Fold [3]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 3.28 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 53.9 ms per loop\n",
        "  Fold [4]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.4 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 56.5 ms per loop\n",
        "Done training classifiers.\n",
        "\n",
        "Artificial Neural Network Accuracy = 0.946666666667\n",
        "Random Forest Accuracy = 0.94\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that even for this small dataset, an artificial neural network with one hidden layer takes a comparatively long time to train. While the implementation is undoubtly less optimized than algorithms available via scikit-learn, the current artificial neural network models are known to be comparatively slow. At the same time, the are also known for generating comparatively accurate predictions."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
