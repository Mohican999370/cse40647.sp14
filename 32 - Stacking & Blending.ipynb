{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stacking/Blending"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stacking (sometimes called stacked generalization or blending) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. In practice, a single-layer logistic regression model is often used as the combiner, although stacking can theoretically represent a variety of ensemble techniques by using any arbitrary combiner algorithm. Stacking/blending typically yields performance better than any single one of the trained models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Reid Johnson\n",
      "#\n",
      "# Generates a stacking/blending of base models. Cross-validation is used to \n",
      "# generate predictions from base (level 0) models that are used as input to a \n",
      "# combiner (level 1) model.\n",
      "#\n",
      "# The stacked generalization (stacking/blending) scheme is defined by Wolpert in:\n",
      "# Stacked generalization.\n",
      "# Neural networks 5.2 (1992): 241-259.\n",
      "\n",
      "import copy\n",
      "\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "class Blending(object):\n",
      "    def __init__(self,\n",
      "                 n_folds=5,\n",
      "                 shuffle=False,\n",
      "                 proba=False,\n",
      "                 bclf=LogisticRegression(), \n",
      "                 clfs=[RandomForestClassifier(n_estimators=10),\n",
      "                       ExtraTreesClassifier(n_estimators=10),\n",
      "                       GradientBoostingClassifier(n_estimators=10)],\n",
      "                 VERBOSE=False\n",
      "                 ):\n",
      "        self.n_folds = n_folds # (int) number of folds for cross-validation\n",
      "        self.shuffle = shuffle # (bool) shuffle the input data (prior to generating folds)\n",
      "        self.proba = proba # (bool) combine predicted label probabilities (rather than predicted label values)\n",
      "        self.bclf = bclf # (obj) combiner (level 1) classifier object\n",
      "        self.clfs = np.empty([len(clfs), n_folds], dtype=object) # (array) initialize array for base (level 0) classifier(s)\n",
      "        self.VERBOSE = VERBOSE # (bool) verbose output\n",
      "\n",
      "    def _gen_skf(self, X, y):\n",
      "        ''' Generates the stratified k-fold cross-validation sets.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output.\n",
      "          y ((n,1) array): The actual outputs (class data).\n",
      "\n",
      "        Returns:\n",
      "          skf: The train/test split as arrays of indices.\n",
      "\n",
      "        '''\n",
      "        if self.shuffle:\n",
      "            np.random.seed(0) # seed to shuffle the training set\n",
      "\n",
      "            idx = np.random.permutation(y.size)\n",
      "            X = X[idx]\n",
      "            y = y[idx]\n",
      "\n",
      "        # Generate k stratified folds of the training data.\n",
      "        skf = list(cross_validation.StratifiedKFold(y, self.n_folds))\n",
      "\n",
      "        return skf\n",
      "\n",
      "    def _base_fit(self, X, y, skf):\n",
      "        ''' Fit the model given predictor(s) X and target y.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output.\n",
      "          y ((n,1) array): The actual outputs (class data).\n",
      "          skf (iterator): The train/test split as arrays of indices.\n",
      "\n",
      "        Returns:\n",
      "          b_X: The mode of the label predictions generated by the specified base model \n",
      "                for each fold for each instance X.\n",
      "\n",
      "        '''\n",
      "        if self.VERBOSE:\n",
      "            print 'Training and validating the base (level 0) classifier(s)...'\n",
      "\n",
      "        # Generate arrays for meta-level training and testing sets, which are n x len(clfs).\n",
      "        b_X_train = np.zeros((X.shape[0], len(clfs))) # meta-level train\n",
      "\n",
      "        # For each classifier, train the number of fold times (=len(skf)).\n",
      "        for j, clf in enumerate(clfs):\n",
      "            if self.VERBOSE:\n",
      "                print 'Training classifier [%s]' % (j)\n",
      "\n",
      "            # Iterate over the folds, each with training set and validation set indicies.\n",
      "            for i, (train_ix, valid_ix) in enumerate(skf):\n",
      "                if self.VERBOSE:\n",
      "                    print '  Fold [%s]' % (i)\n",
      "\n",
      "                # Generate the training set for the fold.\n",
      "                X_train = X[train_ix]\n",
      "                y_train = y[train_ix]\n",
      "\n",
      "                # Generate the validation set for the fold.\n",
      "                X_valid = X[valid_ix]\n",
      "                y_valid = y[valid_ix]\n",
      "\n",
      "                # Fit the base model using the training set for the fold.\n",
      "                clf.fit(X_train, y_train)\n",
      "\n",
      "                if self.proba:\n",
      "                    # Predict the class probabilities for the validation set features.\n",
      "                    b_X_train[valid_ix,j] = clf.predict_proba(X_valid)\n",
      "                else:\n",
      "                    # Predict the class values for the validation set features.\n",
      "                    b_X_train[valid_ix,j] = clf.predict(X_valid)\n",
      "\n",
      "                # Copy the fitted classifier object to the base classifier array.\n",
      "                self.clfs[j,i] = copy.deepcopy(clf)\n",
      "\n",
      "        if self.VERBOSE:\n",
      "            print 'Done training and validating the base (level 0) classifier(s).'\n",
      "\n",
      "        return b_X_train\n",
      "\n",
      "    def _base_clf_predict(self, X, j):\n",
      "        ''' Predict labels with the specified fitted model on predictor(s) X.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted outputs.\n",
      "          j (int): The index of the fitted base model with which to generate predictions.\n",
      "\n",
      "        Returns:\n",
      "          b_X: The mode of the label predicted by the specified base model for each fold \n",
      "                for each instance X.\n",
      "\n",
      "        '''\n",
      "        # Generate array for the base-level testing set, which is n x n_folds.\n",
      "        b_X_j = np.zeros((X.shape[0], self.n_folds))\n",
      "        for i in range(self.n_folds):\n",
      "            # Predict the label values for the testing set features.\n",
      "            b_X_j[:,i] = self.clfs[j,i].predict(X)\n",
      "\n",
      "        # The mode of the base model's label predictions on the testing set.\n",
      "        return sp.stats.mode(b_X_j, axis=1)[0][:,0]\n",
      "\n",
      "    def _base_clf_predict_proba(self, X, j):\n",
      "        ''' Predict probabilities with the specified fitted model on predictor(s) X.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted outputs.\n",
      "          j (int): The index of the fitted base model with which to generate predictions.\n",
      "\n",
      "        Returns:\n",
      "          b_X: The mean of the probabilities predictioned by the specified base model for \n",
      "                each fold for each instance X.\n",
      "\n",
      "        '''\n",
      "        # Generate array for the base-level testing set, which is n x n_folds.\n",
      "        b_X_j = np.zeros((X.shape[0], self.n_folds))\n",
      "        for i in range(self.n_folds):\n",
      "            # Predict the label probabilities for the testing set features.\n",
      "            b_X_j[:,i] = self.clfs[j,i].predict_proba(X)\n",
      "\n",
      "        # The mean of the base model's probability predictions on the testing set.\n",
      "        return b_X_j.mean(axis=1)\n",
      "\n",
      "    def _base_predict(self, X):\n",
      "        ''' Predict labels with the fitted models on predictor(s) X.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted outputs.\n",
      "\n",
      "        Returns:\n",
      "          b_X: The (mode of the) labels predicted by each base model on each instance X.\n",
      "\n",
      "        '''\n",
      "        # Generate array for meta-level testing set, which is n x len(clfs).\n",
      "        b_X = np.zeros((X.shape[0], len(clfs))) # meta-level test\n",
      "\n",
      "        # For each classifier, generate predictions on the testing set.\n",
      "        for j in range(len(self.clfs)):\n",
      "            if self.VERBOSE:\n",
      "                 print 'Predicting testing set labels with classifier [%s].' % (j)\n",
      "\n",
      "            # Use as the blending testing set features.\n",
      "            b_X[:,j] = self._base_clf_predict(X, j)\n",
      "\n",
      "        return b_X\n",
      "\n",
      "    def _base_predict_proba(self, X):\n",
      "        ''' Predict probabilities with the fitted model on predictor(s) X.\n",
      "\n",
      "        The probabilities are computed as the output activation of units in the output layer.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output probabilities.\n",
      "\n",
      "        Returns:\n",
      "          b_X: The (mean of the) probabilities predicted by each base model on each instance X.\n",
      "\n",
      "        '''\n",
      "        # Generate array for meta-level testing set, which is n x len(clfs).\n",
      "        b_X = np.zeros((X.shape[0], len(clfs))) # meta-level test\n",
      "\n",
      "        # For each classifier, generate predictions on the testing set.\n",
      "        for j, clf in enumerate(self.clfs):\n",
      "            if self.VERBOSE:\n",
      "                print 'Predicting testing set probabilities with classifier [%s].' % (j)\n",
      "\n",
      "            # Predict the classes for the testing set features.\n",
      "            b_X[:,j] = self._base_clf_predict_proba(X, j)\n",
      "\n",
      "        return b_X\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        ''' Fit the model given predictor(s) X and target y.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output.\n",
      "          y ((n,1) array): The actual outputs (class data).\n",
      "\n",
      "        '''\n",
      "        if self.VERBOSE:\n",
      "            print \"Training the blended (level 1) classifier...\"\n",
      "\n",
      "        # Generate crossfold-validation iterator.\n",
      "        skf = self._gen_skf(X, y)\n",
      "\n",
      "        # Generate training set features for the meta (level 1) classifier.\n",
      "        b_X = self._base_fit(X, y, skf)\n",
      "\n",
      "        # Blend it like Beckham!\n",
      "        self.bclf.fit(b_X, y)\n",
      "\n",
      "        if self.VERBOSE:\n",
      "            print 'Done training the blended (level 1) classifier.'\n",
      "\n",
      "    def predict(self, X):\n",
      "        ''' Predict labels with the fitted model on predictor(s) X.\n",
      "\n",
      "        Return:\n",
      "          The predicted labels for each instance X.\n",
      "        '''\n",
      "        # Generate testing set features for the meta (level 1) classifier.\n",
      "        b_X = self._base_predict(X)\n",
      "\n",
      "        return self.bclf.predict(b_X)\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        ''' Predict label probabilities with the fitted model on predictor(s) X.\n",
      "\n",
      "        Return:\n",
      "          The predicted probabilities for each instance X.\n",
      "        '''\n",
      "        # Generate testing set features for the meta (level 1) classifier.\n",
      "        b_X = self._base_predict_proba(X)\n",
      "\n",
      "        return self.bclf.predict_proba(b_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we will demonstrate stacking/blending by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding method available via scikit-learn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import preprocessing\n",
      "\n",
      "label_encode = True\n",
      "\n",
      "# Load the Iris flower dataset\n",
      "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
      "iris = pd.read_csv(fileURL, \n",
      "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
      "                   header=None)\n",
      "iris = iris.dropna()\n",
      "\n",
      "X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] # features\n",
      "y = iris['Species'] # class\n",
      "\n",
      "if label_encode:\n",
      "    # Transform string (nominal) output to numeric\n",
      "    labels = preprocessing.LabelEncoder().fit_transform(y)\n",
      "else:\n",
      "    labels = y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we generate the base (level 0) classifiers, which are the classifiers whose predictions on the training data will be combined by a higher-level (level 1) classifier. Here, we use different variants of decision trees as our base classifiers, specifically random forest and extra trees, both of which are decision tree ensembles (collections of individual decision trees). By default, we use 10 trees for each ensemble. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
      "\n",
      "n_trees = 10\n",
      "\n",
      "# Generate a list of base (level 0) classifiers.\n",
      "clfs = [RandomForestClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
      "        ExtraTreesClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
      "        #GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=n_trees)\n",
      "        ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we partition the dataset into non-overlapping training and testing sets, with 60% of the data allocated to the training set and 40% allocated to the testing set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "\n",
      "# The training sets will be used for all training and validation purposes.\n",
      "# The testing sets will only be used for evaluating the final blended (level 1) classifier.\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, labels, test_size=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The concept of stacking requires that the base classifiers generate output that can be further processed by a higher-level classifier. To generate this output, the base classifiers must produce predictions on some sort of testing data. The base classifiers cannot use the original testing set, as these predictions will be used to train the higher-level classifier, and the original testing data should not be used to influence model training. Thus, the training set must itself be divided into training and testing portions for the base classifiers, which can be accomplished by cross-validation.\n",
      "\n",
      "Here, we use 5-fold cross-validation to partition the training set into five non-overlapping sets or folds. Note that the folds we generate are stratified, which means that each fold contains roughly the same proportion of each class label. On each iteration of cross-validation, one fold is used for validation while the remaining folds are used for training. The predictions generated by the base classifiers on the validation set are used as training features or predictor variables input to the higher-level classifier, while the predictions generated by the base classifiers on the original testing set are used as testing features or predictor variables input to the higher-level classifier. The labels (the class or target variable) remain the same. Thus, you can think of this process as replacing the original feature values for each instance by the predictions made by each classifier. By using cross-validation, we are able to use the original training portion of the dataset to both train and evaluate our base classifiers, which allows us to obtain predictions over (and thus generate new feature values for) all of the training instances. Since our higher-level classifier will be trained on these new feature values, we must also replace the original feature values for the testing data with the base model predictions on the testing data. As these preditions can be generated over the entire testing set on each fold, the average predictions over all folds are used as testing input to the higher-level model.\n",
      "\n",
      "After we have trained the base classifiers on the training set, we stack/blend them. This means that we use the outputs (predictions) from the base (level 0) classifiers as input to a higher-level (level 1) classifier. Here, we use logistic regression as the higher-level classifier. As a result, the output generated by the logistic regression model, which is trained or fit on the predictions of the lower-level models, is used to predict the target or class variable of interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "bl = Blending(n_folds=5, shuffle=False, proba=True, bclf=LogisticRegression(), clfs=clfs, VERBOSE=True)\n",
      "bl.fit(X_train, y_train)\n",
      "\n",
      "y_pred = bl.predict(X_test)\n",
      "print y_pred\n",
      "\n",
      "score = metrics.accuracy_score(y_test, y_pred)\n",
      "print\n",
      "print 'Blended Classifier Accuracy Proba-NonProba = %s' % (score)\n",
      "print\n",
      "\n",
      "y_pred = bl.predict_proba(X_test)\n",
      "print y_pred\n",
      "\n",
      "score = metrics.accuracy_score(y_test, y_pred)\n",
      "print\n",
      "print 'Blended Classifier Accuracy Proba-Proba = %s' % (score)\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the blended (level 1) classifier...\n",
        "Training and validating the base (level 0) classifier(s)...\n",
        "Training classifier [0]\n",
        "  Fold [0]\n",
        "  Fold [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [0]\n",
        "  Fold [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done training and validating the base (level 0) classifier(s)."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done training the blended (level 1) classifier.\n",
        "Predicting testing set labels with classifier [0].\n",
        "Predicting testing set labels with classifier [1]."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
        " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Blended Classifier Accuracy Proba-NonProba = 0.0\n",
        "\n",
        "Predicting testing set probabilities with classifier [0].\n",
        "Predicting testing set probabilities with classifier [1]."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.86799828  0.07681234  0.05518939]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90089127  0.05714833  0.0419604 ]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.09089091  0.49732168  0.41178741]\n",
        " [ 0.87891826  0.07040046  0.05068128]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.88567372  0.06631917  0.04800711]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.88567372  0.06631917  0.04800711]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90089127  0.05714833  0.0419604 ]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.90367264  0.05547612  0.04085124]\n",
        " [ 0.89209012  0.06179642  0.04611347]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.08131991  0.50068557  0.41799452]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.07700677  0.50289015  0.42010308]\n",
        " [ 0.90367264  0.05547612  0.04085124]]"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Can't handle mix of multiclass and continuous-multioutput",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-774083d8c65b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Blended Classifier Accuracy Proba-Proba = %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Reid\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_clf_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1065\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multilabel-indicator'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Reid\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.pyc\u001b[0m in \u001b[0;36m_check_clf_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[1;32m--> 115\u001b[1;33m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Can't handle mix of multiclass and continuous-multioutput"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "\n",
      "bl = Blending(n_folds=5, shuffle=False, proba=False, bclf=LogisticRegression(), clfs=clfs, VERBOSE=True)\n",
      "bl.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can compare the performance of the ensemble of blended classifiers to that of the individual (base) ones:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Generate predictions with blended (level 1) classifier. ###\n",
      "\n",
      "y_pred = bl.predict(X_test)\n",
      "print y_pred\n",
      "\n",
      "score = metrics.accuracy_score(y_test, y_pred)\n",
      "print\n",
      "print 'Blended Classifier Accuracy = %s' % (score)\n",
      "print\n",
      "\n",
      "### Generate predictions with base (level 0) classifiers. ###\n",
      "\n",
      "# Random forest predictions.\n",
      "score0 = metrics.accuracy_score(y_test, bl._base_clf_predict(X_test, 0))\n",
      "print 'Random Forest (10 trees) Accuracy = %s' % (score0)\n",
      "\n",
      "# Extra trees predictions.\n",
      "score1 = metrics.accuracy_score(y_test, bl._base_clf_predict(X_test, 1))\n",
      "print 'Extra Trees (10 trees) Accuracy = %s' % (score1)\n",
      "\n",
      "# Gradient boosted trees predictions.\n",
      "#score2 = metrics.accuracy_score(y_test, bl._base_clf_predict(X_test, 2))\n",
      "#print 'Gradient Boosted Trees (10 trees) Accuracy = %s' % (score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that by using the method of stacking/blending, we generate a \"meta\" classifier that can outperform each of the base classifiers."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
