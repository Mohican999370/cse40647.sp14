{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stacking/Blending"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stacking (sometimes called stacked generalization or blending) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. In practice, a single-layer logistic regression model is often used as the combiner, although stacking can theoretically represent a variety of ensemble techniques by using any arbitrary combiner algorithm. Stacking/blending typically yields performance better than any single one of the trained models.\n",
      "\n",
      "Here, we will demonstrate stacking/blending by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding feature method via scikit-learn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Reid Johnson\n",
      "#\n",
      "# Generates a stacking/blending of base models. Cross-validation is used to \n",
      "# generate predictions from base (level 0) models that are used as input to a \n",
      "# combiner (level 1) model.\n",
      "#\n",
      "# The stacked generalization (stacking/blending) scheme is defined by Wolpert in:\n",
      "# Stacked generalization.\n",
      "# Neural networks 5.2 (1992): 241-259.\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import preprocessing\n",
      "\n",
      "label_encode = True\n",
      "\n",
      "# Load the Iris flower dataset\n",
      "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
      "iris = pd.read_csv(fileURL, \n",
      "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
      "                   header=None)\n",
      "iris = iris.dropna()\n",
      "\n",
      "X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] # features\n",
      "y = iris['Species'] # class\n",
      "\n",
      "if label_encode:\n",
      "    # Transform string (nominal) output to numeric\n",
      "    labels = preprocessing.LabelEncoder().fit_transform(y)\n",
      "else:\n",
      "    labels = y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we generate the base (level 0) classifiers, which are the classifiers whose predictions on the training data will be combined by a higher-level (level 1) classifier. Here, we use different variants of decision trees as our base classifiers, specifically random forest and extra trees, both of which are decision tree ensembles (collections of individual decision trees). By default, we use 10 trees for each ensemble. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
      "\n",
      "n_trees = 10\n",
      "\n",
      "# Generate a list of base (level 0) classifiers.\n",
      "clfs = [RandomForestClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
      "        ExtraTreesClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
      "        #GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=n_trees)\n",
      "        ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The concept of stacking requires that the base classifiers generate output that can be further processed by a higher-level classifier. To generate this output, the base classifiers must produce predictions on some sort of testing data. The base classifiers cannot use the original testing set, as these predictions will be used to train the higher-level classifier, and the original testing data should not be used to influence model training. Thus, the training set must itself be divided into training and testing portions for the base classifiers, which can be accomplished by cross-validation.\n",
      "\n",
      "Here, we first partition the dataset into non-overlapping training and testing sets, with 60% of the data allocated to the training set and 40% allocated to the testing set. We then use 5-fold cross-validation to further partition the training set into five non-overlapping sets or folds. Note that the folds we generate are stratified, which means that each fold contains roughly the same proportion of each class label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "\n",
      "# The training sets will be used for all training and validation purposes.\n",
      "# The testing sets will only be used for evaluating the final blended (level 1) classifier.\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, labels, test_size=0.4)\n",
      "\n",
      "n_folds = 5\n",
      "shuffle = False\n",
      "\n",
      "if shuffle:\n",
      "    np.random.seed(0) # seed to shuffle the training set\n",
      "\n",
      "    idx = np.random.permutation(y_train.size)\n",
      "    X_train = X_train[idx]\n",
      "    y_train = y_train[idx]\n",
      "\n",
      "# Generate k stratified folds of the training data.\n",
      "skf = list(cross_validation.StratifiedKFold(y_train, n_folds))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On each iteration of cross-validation, one fold is used for validation while the remaining folds are used for training. The predictions generated by the base classifiers on the validation set are used as training features or predictor variables input to the higher-level classifier, while the predictions generated by the base classifiers on the original testing set are used as testing features or predictor variables input to the higher-level classifier. The labels (the class or target variable) remain the same. Thus, you can think of this process as replacing the original feature values for each instance by the predictions made by each classifier. By using cross-validation, we are able to use the original training portion of the dataset to both train and evaluate our base classifiers, which allows us to obtain predictions over (and thus generate new feature values for) all of the training instances. Since our higher-level classifier will be trained on these new feature values, we must also replace the original feature values for the testing data with the base model predictions on the testing data. As these preditions can be generated over the entire testing set on each fold, the average predictions over all folds are used as testing input to the higher-level model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Training and validating the base (level 0) classifiers...\"\n",
      "\n",
      "# Generate arrays for meta-level training and testing sets, which are n x len(clfs).\n",
      "blend_train = np.zeros((X_train.shape[0], len(clfs))) # meta-level train\n",
      "blend_test = np.zeros((X_test.shape[0], len(clfs))) # meta-level test\n",
      "\n",
      "#print 'X_test.shape = %s' % (str(X_test.shape))\n",
      "#print 'blend_train.shape = %s' % (str(blend_train.shape))\n",
      "#print 'blend_test.shape = %s' % (str(blend_test.shape))\n",
      "\n",
      "# For each classifier, train the number of fold times (=len(skf)).\n",
      "for j, clf in enumerate(clfs):\n",
      "    print 'Training classifier [%s]' % (j)\n",
      "\n",
      "    # Generate array for the testing set, which is n x len(skf).\n",
      "    blend_test_j = np.zeros((X_test.shape[0], len(skf)))\n",
      "\n",
      "    # Iterate over the folds, each with training set and validation set indicies.\n",
      "    for i, (train_index, valid_index) in enumerate(skf):\n",
      "        print '  Fold [%s]' % (i)\n",
      "\n",
      "        # Generate the training set for the fold.\n",
      "        X_blend_train = X_train[train_index]\n",
      "        y_blend_train = y_train[train_index]\n",
      "\n",
      "        # Generate the validation set for the fold.\n",
      "        X_blend_valid = X_train[valid_index]\n",
      "        y_blend_valid = y_train[valid_index]\n",
      "\n",
      "        # Fit the base model using the training set for the fold.\n",
      "        clf.fit(X_blend_train, y_blend_train)\n",
      "\n",
      "        # Predict the class for the validation set features.\n",
      "        # Use as the blending training set features.\n",
      "        blend_train[valid_index,j] = clf.predict(X_blend_valid)\n",
      "\n",
      "        # Predict the classes for the testing set features.\n",
      "        blend_test_j[:,i] = clf.predict(X_test)\n",
      "\n",
      "    # The mean of the predictions on the testing set.\n",
      "    # Use as the blending testing set features.\n",
      "    blend_test[:,j] = blend_test_j.mean(axis=1)\n",
      "\n",
      "#print 'y_train.shape = %s' % (y_train.shape)\n",
      "\n",
      "print \"Done training and validating the base (level 0) classifiers.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training and validating the base (level 0) classifiers...\n",
        "Training classifier [0]\n",
        "  Fold [0]\n",
        "  Fold [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training classifier [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [0]\n",
        "  Fold [1]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  Fold [4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done training and validating the base (level 0) classifiers."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we stack or blend the base classifiers. This means that we use the outputs (predictions) from the base (level 0) classifiers as input to a higher-level (level 1) classifier. Here, we use logistic regression as the higher-level classifier. As a result, the output generated by the logistic regression model, which is trained on the predictions of the lower-level models, is used to predict the target or class variable of interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "print \"Training the blended (level 1) classifier...\",\n",
      "\n",
      "# Blend it like Beckham!\n",
      "bclf = LogisticRegression()\n",
      "bclf.fit(blend_train, y_train)\n",
      "\n",
      "print \"done.\"\n",
      "\n",
      "# Predict\n",
      "y_test_pred = bclf.predict(blend_test)\n",
      "score = metrics.accuracy_score(y_test, y_test_pred)\n",
      "print 'Blended Classifier Accuracy = %s' % (score)\n",
      "\n",
      "# Generate non-blended classifiers.\n",
      "clf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy')\n",
      "clf1.fit(X_train, y_train)\n",
      "clf2 = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy')\n",
      "clf2.fit(X_train, y_train)\n",
      "\n",
      "# Predict\n",
      "y_test_pred1 = clf1.predict(X_test)\n",
      "y_test_pred2 = clf2.predict(X_test)\n",
      "score1 = metrics.accuracy_score(y_test, y_test_pred1)\n",
      "score2 = metrics.accuracy_score(y_test, y_test_pred2)\n",
      "print 'Random Forest (100 trees) Accuracy = %s' % (score1)\n",
      "print 'Extra Trees (100 trees) Accuracy = %s' % (score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the blended (level 1) classifier... done.\n",
        "Blended Classifier Accuracy = 0.95\n",
        "Random Forest (100 trees) Accuracy = 0.933333333333"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Extra Trees (100 trees) Accuracy = 0.933333333333\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that by stacking or blending the classifiers (each with only 10 trees), we can generate a \"meta\" classifier that outperforms the individual classifiers (each with 100 trees)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
